# Project NLP MVA 23 ğŸ“š

Welcome to the repoitory of the final project for the Algorithms for Speech and Natural Language Processing class.

## Datasets ğŸ“Š

This section contains various datasets utilized for training our models.

## Code ğŸ‘©â€ğŸ’»

- **Outputs:** Explore our project's code and outputs:
  - [speed_test](#)
  - [content/Project_NLP_MVA_23](#)
  - [pet-master](#)
 

## Final report 
  - [NLPMVA23](#)


## Pattern Exploitation Training (PET) ğŸ§ 

### Abstract
Our project is inspired by "Itâ€™s not just size that matters" by Schick and SchÃ¼tze (2020), introducing Pattern Exploitation Training (PET). PET reframes tasks as language modeling problems, using fine-tuned language models to label unlabeled data. This enables classical classifiers with small training datasets and supports few-shot learning. Our project explores PET, replicates its results on various datasets, and compares different masked language models.

### Introduction
Language models like GPT-3 and GPT-4 excel in natural language processing but require substantial computational resources. PET offers an efficient alternative by transforming tasks into language modeling challenges. It uses predefined patterns and verbalizers to convert features into sentences and labels into words. PET facilitates few-shot learning and works well with small training datasets, leveraging easily accessible unlabeled data.

### Project Scope and Findings
Our project replicates Schick and SchÃ¼tze's study and compares masked language models. While some SuperGLUE tasks showed lower accuracies, sentiment classification tasks performed well. We faced challenges such as labeling errors for specific tasks. Additionally, we created new tasks and explored MLM performance within a few-shot learning paradigm.

### Conclusion
In conclusion, our project provides insights into the effectiveness of Pattern Exploitation Training (PET) for natural language tasks. Despite limitations due to hardware and time constraints, our findings contribute to the discussion on efficient language model training and real-world applications.

## Links ğŸ”—

### Article ğŸ“„
- [Read Article 1](https://arxiv.org/pdf/2001.07676.pdf)
- [Read Article 2](https://arxiv.org/pdf/2009.07118.pdf)

### Google Colab ğŸš€
- [Access Google Colab](https://colab.research.google.com/drive/1zd60dwooww8VV0NCRib-pO9FdeyT01Jv#scrollTo=wxfFVBPrZ5lZ)

### Dataset MNLI ğŸ“¦
- [Download MNLI Dataset](https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip)

### List of all used datasets ğŸ“‹
- [Explore Datasets](https://paperswithcode.com/paper/it-s-not-just-size-that-matters-small)

### Important YouTube Videos ğŸ¥
- [Watch Video 1](https://www.youtube.com/watch?v=P7Rav5tK3Y0)
- [Watch Video 2](https://youtu.be/01jRE9noSWw)
